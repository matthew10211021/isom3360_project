{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mining Polish Bankruptcy Data\n",
    "<table><tr><td><img src='resources/img_0.jpg'></td><td><img src='resources/img_1.jpg'></td><td><img src='resources/img_2.jpg'></td></tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Import libraries and data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (i) libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general utilities\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.io import arff\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# classification models\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# for hyperparameters tuning and cross validation\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# for displaying / plotting\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (ii) data\n",
    "- loading data from arff file<br>\n",
    "Reference: https://discuss.analyticsvidhya.com/t/loading-arff-type-files-in-python/27419"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import data\n",
    "years = [1,2,3,4,5]\n",
    "raw_data = {} # dictionary of dataframe\n",
    "\n",
    "for year in years:\n",
    "    arr = arff.loadarff(f\"data/year{year}.arff\")\n",
    "    raw_data[f\"year{str(year)}\"] = pd.DataFrame(arr[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Attr1</th>\n",
       "      <th>Attr2</th>\n",
       "      <th>Attr3</th>\n",
       "      <th>Attr4</th>\n",
       "      <th>Attr5</th>\n",
       "      <th>Attr6</th>\n",
       "      <th>Attr7</th>\n",
       "      <th>Attr8</th>\n",
       "      <th>Attr9</th>\n",
       "      <th>Attr10</th>\n",
       "      <th>...</th>\n",
       "      <th>Attr56</th>\n",
       "      <th>Attr57</th>\n",
       "      <th>Attr58</th>\n",
       "      <th>Attr59</th>\n",
       "      <th>Attr60</th>\n",
       "      <th>Attr61</th>\n",
       "      <th>Attr62</th>\n",
       "      <th>Attr63</th>\n",
       "      <th>Attr64</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.200550</td>\n",
       "      <td>0.37951</td>\n",
       "      <td>0.39641</td>\n",
       "      <td>2.04720</td>\n",
       "      <td>32.3510</td>\n",
       "      <td>0.388250</td>\n",
       "      <td>0.249760</td>\n",
       "      <td>1.330500</td>\n",
       "      <td>1.13890</td>\n",
       "      <td>0.504940</td>\n",
       "      <td>...</td>\n",
       "      <td>0.121960</td>\n",
       "      <td>0.397180</td>\n",
       "      <td>0.87804</td>\n",
       "      <td>0.001924</td>\n",
       "      <td>8.4160</td>\n",
       "      <td>5.1372</td>\n",
       "      <td>82.658</td>\n",
       "      <td>4.4158</td>\n",
       "      <td>7.42770</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.209120</td>\n",
       "      <td>0.49988</td>\n",
       "      <td>0.47225</td>\n",
       "      <td>1.94470</td>\n",
       "      <td>14.7860</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.258340</td>\n",
       "      <td>0.996010</td>\n",
       "      <td>1.69960</td>\n",
       "      <td>0.497880</td>\n",
       "      <td>...</td>\n",
       "      <td>0.121300</td>\n",
       "      <td>0.420020</td>\n",
       "      <td>0.85300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.1486</td>\n",
       "      <td>3.2732</td>\n",
       "      <td>107.350</td>\n",
       "      <td>3.4000</td>\n",
       "      <td>60.98700</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.248660</td>\n",
       "      <td>0.69592</td>\n",
       "      <td>0.26713</td>\n",
       "      <td>1.55480</td>\n",
       "      <td>-1.1523</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.309060</td>\n",
       "      <td>0.436950</td>\n",
       "      <td>1.30900</td>\n",
       "      <td>0.304080</td>\n",
       "      <td>...</td>\n",
       "      <td>0.241140</td>\n",
       "      <td>0.817740</td>\n",
       "      <td>0.76599</td>\n",
       "      <td>0.694840</td>\n",
       "      <td>4.9909</td>\n",
       "      <td>3.9510</td>\n",
       "      <td>134.270</td>\n",
       "      <td>2.7185</td>\n",
       "      <td>5.20780</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.081483</td>\n",
       "      <td>0.30734</td>\n",
       "      <td>0.45879</td>\n",
       "      <td>2.49280</td>\n",
       "      <td>51.9520</td>\n",
       "      <td>0.149880</td>\n",
       "      <td>0.092704</td>\n",
       "      <td>1.866100</td>\n",
       "      <td>1.05710</td>\n",
       "      <td>0.573530</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054015</td>\n",
       "      <td>0.142070</td>\n",
       "      <td>0.94598</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.5746</td>\n",
       "      <td>3.6147</td>\n",
       "      <td>86.435</td>\n",
       "      <td>4.2228</td>\n",
       "      <td>5.54970</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.187320</td>\n",
       "      <td>0.61323</td>\n",
       "      <td>0.22960</td>\n",
       "      <td>1.40630</td>\n",
       "      <td>-7.3128</td>\n",
       "      <td>0.187320</td>\n",
       "      <td>0.187320</td>\n",
       "      <td>0.630700</td>\n",
       "      <td>1.15590</td>\n",
       "      <td>0.386770</td>\n",
       "      <td>...</td>\n",
       "      <td>0.134850</td>\n",
       "      <td>0.484310</td>\n",
       "      <td>0.86515</td>\n",
       "      <td>0.124440</td>\n",
       "      <td>6.3985</td>\n",
       "      <td>4.3158</td>\n",
       "      <td>127.210</td>\n",
       "      <td>2.8692</td>\n",
       "      <td>7.89800</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7022</th>\n",
       "      <td>0.018371</td>\n",
       "      <td>0.47410</td>\n",
       "      <td>-0.13619</td>\n",
       "      <td>0.60839</td>\n",
       "      <td>-18.4490</td>\n",
       "      <td>0.018371</td>\n",
       "      <td>0.018371</td>\n",
       "      <td>0.972030</td>\n",
       "      <td>1.01210</td>\n",
       "      <td>0.460840</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011909</td>\n",
       "      <td>0.039866</td>\n",
       "      <td>0.98809</td>\n",
       "      <td>0.274140</td>\n",
       "      <td>73.5050</td>\n",
       "      <td>79.2370</td>\n",
       "      <td>31.268</td>\n",
       "      <td>11.6730</td>\n",
       "      <td>5.14890</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7023</th>\n",
       "      <td>-0.013359</td>\n",
       "      <td>0.58354</td>\n",
       "      <td>-0.02265</td>\n",
       "      <td>0.92896</td>\n",
       "      <td>-42.2320</td>\n",
       "      <td>-0.013359</td>\n",
       "      <td>-0.015036</td>\n",
       "      <td>0.562890</td>\n",
       "      <td>0.98904</td>\n",
       "      <td>0.328470</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011082</td>\n",
       "      <td>-0.040671</td>\n",
       "      <td>1.01110</td>\n",
       "      <td>0.805920</td>\n",
       "      <td>10.5990</td>\n",
       "      <td>7.1740</td>\n",
       "      <td>94.092</td>\n",
       "      <td>3.8792</td>\n",
       "      <td>1.75720</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7024</th>\n",
       "      <td>0.006338</td>\n",
       "      <td>0.50276</td>\n",
       "      <td>0.43923</td>\n",
       "      <td>1.87360</td>\n",
       "      <td>9.7417</td>\n",
       "      <td>0.006338</td>\n",
       "      <td>0.012022</td>\n",
       "      <td>0.983560</td>\n",
       "      <td>1.00830</td>\n",
       "      <td>0.494490</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008258</td>\n",
       "      <td>0.012817</td>\n",
       "      <td>0.99174</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.4700</td>\n",
       "      <td>6.0759</td>\n",
       "      <td>51.019</td>\n",
       "      <td>7.1542</td>\n",
       "      <td>62.00100</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7025</th>\n",
       "      <td>-0.041643</td>\n",
       "      <td>0.84810</td>\n",
       "      <td>-0.12852</td>\n",
       "      <td>0.57485</td>\n",
       "      <td>-121.9200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.036795</td>\n",
       "      <td>0.179010</td>\n",
       "      <td>0.42138</td>\n",
       "      <td>0.151820</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.232720</td>\n",
       "      <td>-0.274290</td>\n",
       "      <td>0.98788</td>\n",
       "      <td>3.593100</td>\n",
       "      <td>39.7030</td>\n",
       "      <td>3.1420</td>\n",
       "      <td>261.850</td>\n",
       "      <td>1.3939</td>\n",
       "      <td>0.51005</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7026</th>\n",
       "      <td>0.014946</td>\n",
       "      <td>0.94648</td>\n",
       "      <td>0.03211</td>\n",
       "      <td>1.03630</td>\n",
       "      <td>-20.5810</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015260</td>\n",
       "      <td>0.056357</td>\n",
       "      <td>2.96940</td>\n",
       "      <td>0.053341</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015705</td>\n",
       "      <td>0.280210</td>\n",
       "      <td>0.97443</td>\n",
       "      <td>1.179200</td>\n",
       "      <td>15.0360</td>\n",
       "      <td>4.1741</td>\n",
       "      <td>108.640</td>\n",
       "      <td>3.3599</td>\n",
       "      <td>35.11800</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7027 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Attr1    Attr2    Attr3    Attr4     Attr5     Attr6     Attr7  \\\n",
       "0     0.200550  0.37951  0.39641  2.04720   32.3510  0.388250  0.249760   \n",
       "1     0.209120  0.49988  0.47225  1.94470   14.7860  0.000000  0.258340   \n",
       "2     0.248660  0.69592  0.26713  1.55480   -1.1523  0.000000  0.309060   \n",
       "3     0.081483  0.30734  0.45879  2.49280   51.9520  0.149880  0.092704   \n",
       "4     0.187320  0.61323  0.22960  1.40630   -7.3128  0.187320  0.187320   \n",
       "...        ...      ...      ...      ...       ...       ...       ...   \n",
       "7022  0.018371  0.47410 -0.13619  0.60839  -18.4490  0.018371  0.018371   \n",
       "7023 -0.013359  0.58354 -0.02265  0.92896  -42.2320 -0.013359 -0.015036   \n",
       "7024  0.006338  0.50276  0.43923  1.87360    9.7417  0.006338  0.012022   \n",
       "7025 -0.041643  0.84810 -0.12852  0.57485 -121.9200  0.000000 -0.036795   \n",
       "7026  0.014946  0.94648  0.03211  1.03630  -20.5810  0.000000  0.015260   \n",
       "\n",
       "         Attr8    Attr9    Attr10  ...    Attr56    Attr57   Attr58    Attr59  \\\n",
       "0     1.330500  1.13890  0.504940  ...  0.121960  0.397180  0.87804  0.001924   \n",
       "1     0.996010  1.69960  0.497880  ...  0.121300  0.420020  0.85300  0.000000   \n",
       "2     0.436950  1.30900  0.304080  ...  0.241140  0.817740  0.76599  0.694840   \n",
       "3     1.866100  1.05710  0.573530  ...  0.054015  0.142070  0.94598  0.000000   \n",
       "4     0.630700  1.15590  0.386770  ...  0.134850  0.484310  0.86515  0.124440   \n",
       "...        ...      ...       ...  ...       ...       ...      ...       ...   \n",
       "7022  0.972030  1.01210  0.460840  ...  0.011909  0.039866  0.98809  0.274140   \n",
       "7023  0.562890  0.98904  0.328470  ... -0.011082 -0.040671  1.01110  0.805920   \n",
       "7024  0.983560  1.00830  0.494490  ...  0.008258  0.012817  0.99174  0.000000   \n",
       "7025  0.179010  0.42138  0.151820  ... -0.232720 -0.274290  0.98788  3.593100   \n",
       "7026  0.056357  2.96940  0.053341  ...  0.015705  0.280210  0.97443  1.179200   \n",
       "\n",
       "       Attr60   Attr61   Attr62   Attr63    Attr64  class  \n",
       "0      8.4160   5.1372   82.658   4.4158   7.42770   b'0'  \n",
       "1      4.1486   3.2732  107.350   3.4000  60.98700   b'0'  \n",
       "2      4.9909   3.9510  134.270   2.7185   5.20780   b'0'  \n",
       "3      4.5746   3.6147   86.435   4.2228   5.54970   b'0'  \n",
       "4      6.3985   4.3158  127.210   2.8692   7.89800   b'0'  \n",
       "...       ...      ...      ...      ...       ...    ...  \n",
       "7022  73.5050  79.2370   31.268  11.6730   5.14890   b'1'  \n",
       "7023  10.5990   7.1740   94.092   3.8792   1.75720   b'1'  \n",
       "7024  10.4700   6.0759   51.019   7.1542  62.00100   b'1'  \n",
       "7025  39.7030   3.1420  261.850   1.3939   0.51005   b'1'  \n",
       "7026  15.0360   4.1741  108.640   3.3599  35.11800   b'1'  \n",
       "\n",
       "[7027 rows x 65 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check that the data is correctly load\n",
    "raw_data[\"year1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (iii) renaming the target column\n",
    "- Since the word \"class\" is a reserved keyword in python, better replace it by another word for easier manipulation later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in years:\n",
    "    raw_data[f\"year{year}\"].rename(columns = {\"class\": \"bankrupted\"}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Data preparation and exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (i) Define attributes and target varialbles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "for i in range(1,65):\n",
    "    features.append(f\"Attr{i}\")\n",
    "\n",
    "target = [\"bankrupted\"]\n",
    "\n",
    "X = {}\n",
    "Y = {}\n",
    "\n",
    "for year in years:\n",
    "    X[f\"year{year}\"] = raw_data[f\"year{year}\"][features]\n",
    "    Y[f\"year{year}\"] = raw_data[f\"year{year}\"][target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (ii) Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = {}\n",
    "X_test = {}\n",
    "Y_train = {}\n",
    "Y_test = {}\n",
    "\n",
    "for year in years:\n",
    "    X_train[f\"year{year}\"], X_test[f\"year{year}\"], Y_train[f\"year{year}\"], Y_test[f\"year{year}\"] = train_test_split(X[f\"year{year}\"], Y[f\"year{year}\"], test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the indices for all dataframe\n",
    "for year in years:\n",
    "    X_train[f\"year{year}\"].reset_index(drop=True, inplace=True)\n",
    "    X_test[f\"year{year}\"].reset_index(drop=True, inplace=True)\n",
    "    Y_train[f\"year{year}\"].reset_index(drop=True, inplace=True)\n",
    "    Y_test[f\"year{year}\"].reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (iii) inspect missing values and data type of data\n",
    "Using year 1 train data as an example: <br>\n",
    "- we can observe that the problem of missing values is quite serious, e.g., for Attr37, there are only 3435 Non-Null values out of 5621 entries\n",
    "- all attributes are in floating point data type, which is desirable\n",
    "- however, the datatype of our target (class) is byte string, we need to convert it into binary number (int) - 0 or 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5621 entries, 0 to 5620\n",
      "Data columns (total 64 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   Attr1   5619 non-null   float64\n",
      " 1   Attr2   5619 non-null   float64\n",
      " 2   Attr3   5619 non-null   float64\n",
      " 3   Attr4   5595 non-null   float64\n",
      " 4   Attr5   5613 non-null   float64\n",
      " 5   Attr6   5619 non-null   float64\n",
      " 6   Attr7   5619 non-null   float64\n",
      " 7   Attr8   5600 non-null   float64\n",
      " 8   Attr9   5621 non-null   float64\n",
      " 9   Attr10  5619 non-null   float64\n",
      " 10  Attr11  5594 non-null   float64\n",
      " 11  Attr12  5595 non-null   float64\n",
      " 12  Attr13  5621 non-null   float64\n",
      " 13  Attr14  5619 non-null   float64\n",
      " 14  Attr15  5619 non-null   float64\n",
      " 15  Attr16  5600 non-null   float64\n",
      " 16  Attr17  5600 non-null   float64\n",
      " 17  Attr18  5619 non-null   float64\n",
      " 18  Attr19  5621 non-null   float64\n",
      " 19  Attr20  5621 non-null   float64\n",
      " 20  Attr21  4327 non-null   float64\n",
      " 21  Attr22  5619 non-null   float64\n",
      " 22  Attr23  5621 non-null   float64\n",
      " 23  Attr24  5527 non-null   float64\n",
      " 24  Attr25  5619 non-null   float64\n",
      " 25  Attr26  5600 non-null   float64\n",
      " 26  Attr27  5380 non-null   float64\n",
      " 27  Attr28  5598 non-null   float64\n",
      " 28  Attr29  5619 non-null   float64\n",
      " 29  Attr30  5621 non-null   float64\n",
      " 30  Attr31  5621 non-null   float64\n",
      " 31  Attr32  5592 non-null   float64\n",
      " 32  Attr33  5595 non-null   float64\n",
      " 33  Attr34  5600 non-null   float64\n",
      " 34  Attr35  5619 non-null   float64\n",
      " 35  Attr36  5619 non-null   float64\n",
      " 36  Attr37  3435 non-null   float64\n",
      " 37  Attr38  5619 non-null   float64\n",
      " 38  Attr39  5621 non-null   float64\n",
      " 39  Attr40  5595 non-null   float64\n",
      " 40  Attr41  5553 non-null   float64\n",
      " 41  Attr42  5621 non-null   float64\n",
      " 42  Attr43  5621 non-null   float64\n",
      " 43  Attr44  5621 non-null   float64\n",
      " 44  Attr45  5503 non-null   float64\n",
      " 45  Attr46  5595 non-null   float64\n",
      " 46  Attr47  5600 non-null   float64\n",
      " 47  Attr48  5619 non-null   float64\n",
      " 48  Attr49  5621 non-null   float64\n",
      " 49  Attr50  5600 non-null   float64\n",
      " 50  Attr51  5619 non-null   float64\n",
      " 51  Attr52  5600 non-null   float64\n",
      " 52  Attr53  5598 non-null   float64\n",
      " 53  Attr54  5598 non-null   float64\n",
      " 54  Attr55  5621 non-null   float64\n",
      " 55  Attr56  5621 non-null   float64\n",
      " 56  Attr57  5620 non-null   float64\n",
      " 57  Attr58  5621 non-null   float64\n",
      " 58  Attr59  5620 non-null   float64\n",
      " 59  Attr60  5502 non-null   float64\n",
      " 60  Attr61  5603 non-null   float64\n",
      " 61  Attr62  5621 non-null   float64\n",
      " 62  Attr63  5595 non-null   float64\n",
      " 63  Attr64  5598 non-null   float64\n",
      "dtypes: float64(64)\n",
      "memory usage: 2.7 MB\n"
     ]
    }
   ],
   "source": [
    "X_train[\"year1\"].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill the NaN values with corresponding column mean of the TRAIN SET for both train and test set\n",
    "\n",
    "for year in years:\n",
    "    mean_values = X_train[f\"year{year}\"].mean()\n",
    "    X_train[f\"year{year}\"] = X_train[f\"year{year}\"].fillna(value=mean_values)\n",
    "    X_test[f\"year{year}\"] = X_test[f\"year{year}\"].fillna(value=mean_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year1 train set contains mo missing values\n",
      "year1 test set contains mo missing values\n",
      "year2 train set contains mo missing values\n",
      "year2 test set contains mo missing values\n",
      "year3 train set contains mo missing values\n",
      "year3 test set contains mo missing values\n",
      "year4 train set contains mo missing values\n",
      "year4 test set contains mo missing values\n",
      "year5 train set contains mo missing values\n",
      "year5 test set contains mo missing values\n"
     ]
    }
   ],
   "source": [
    "# double check all dataframes have zero missing values\n",
    "for year in years:\n",
    "    if (X_train[f\"year{year}\"].isna().sum().sum() == 0):\n",
    "        print(f\"year{year} train set contains mo missing values\")\n",
    "    if (X_test[f\"year{year}\"].isna().sum().sum() == 0):\n",
    "        print(f\"year{year} test set contains mo missing values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the target variable \"class\", we need to convert byte into binary variable (0 and 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bytes"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(Y_train[\"year1\"].iloc[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int.from_bytes(Y_train[\"year1\"].iloc[0][0], \"big\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The view and copy properties of pandas is quite complicated. References for the following code to prevent SettingWithCopywarning:\n",
    "- https://www.youtube.com/watch?v=4R4WsDJ-KVc\n",
    "- https://towardsdatascience.com/3-solutions-for-the-setting-with-copy-warning-of-python-pandas-dfe15d62de08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ASCII code for b'0' is 48\n",
    "# ASCII code for b'1' is 49\n",
    "\n",
    "# apply change of datatype on the \"class\" variable\n",
    "\n",
    "for year in years:\n",
    "    train_copy = Y_train[f\"year{year}\"].copy()\n",
    "    test_copy = Y_test[f\"year{year}\"].copy()\n",
    "    train_copy[\"bankrupted\"] = Y_train[f\"year{year}\"].bankrupted.apply(lambda x: 0 if (int.from_bytes(x, \"big\") == 48)  else 1)\n",
    "    test_copy[\"bankrupted\"] = Y_test[f\"year{year}\"].bankrupted.apply(lambda x: 0 if (int.from_bytes(x, \"big\") == 48)  else 1)\n",
    "    Y_train[f\"year{year}\"] = train_copy.copy()\n",
    "    Y_test[f\"year{year}\"] = test_copy.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (iv) inspect the distribution of the target variable\n",
    "Reference: https://stackoverflow.com/questions/10369681/how-to-plot-bar-graphs-with-same-x-coordinates-side-by-side-dodged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- from the graph plotting below, we can observe a seriosu class imbalance in our target variable, while our variable of interest (bankruptcy) is the minority group\n",
    "- for such imbalanced dataset, evaluation metrics such as precision and recall should be emphasized over accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class_0 = []\n",
    "class_1 = []\n",
    "x_axis_label = []\n",
    "\n",
    "for year in years:\n",
    "    class_0.append(Y_train[f\"year{year}\"][\"bankrupted\"].value_counts()[0])\n",
    "    class_1.append(Y_train[f\"year{year}\"][\"bankrupted\"].value_counts()[1])\n",
    "    x_axis_label.append(f\"year{year}\")\n",
    "    \n",
    "# position of bars on x-axis\n",
    "ind = np.arange(5)\n",
    "\n",
    "# figure size\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "# width of bar\n",
    "width = 0.3\n",
    "\n",
    "# Plotting\n",
    "plt.bar(ind, class_0 , width, label=\"Not bankrupted\")\n",
    "plt.bar(ind + width, class_1, width, label=\"Bankrupted\")\n",
    "        \n",
    "plt.xlabel(\"Dataset\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of target variables in the train set\")\n",
    "\n",
    "plt.xticks(ind + width / 2, x_axis_label)\n",
    "\n",
    "# Finding the best position for legends and putting it\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (v) perform normalization\n",
    "- standardization may help the convergence of logistic regression if we perform regularization\n",
    "- however, normalization generally has little effect on Decision Tree Model and Naive Bayes Model\n",
    "- here we use the train set to create the scaler for both train set and test set, since we assume the train set is representable enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create scalers for each year's dataset\n",
    "\n",
    "# a dictionary of scalers\n",
    "z_score_scaler = {}\n",
    "\n",
    "for year in years:\n",
    "    z_score_scaler[f\"year{year}\"] = preprocessing.StandardScaler().fit(X_train[f\"year{year}\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the scalers to each dataset\n",
    "\n",
    "for year in years:\n",
    "    X_train[f\"year{year}\"] = z_score_scaler[f\"year{year}\"].transform(X_train[f\"year{year}\"])\n",
    "    X_test[f\"year{year}\"] = z_score_scaler[f\"year{year}\"].transform(X_test[f\"year{year}\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Data modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are performing a classification task, and our team will try out the following models:\n",
    "- Decision Tree Model\n",
    "- Random Forest Model\n",
    "- Gaussian Naive Bayes Model\n",
    "- Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (i) Decision Tree Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following hyper-paramteres will be used in the decision tree model:\n",
    "- max_depth\n",
    "- max_leaf_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "depths = np.arange(1, 21)\n",
    "max_leafs = np.arange(5,51,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a grid of decision tree hyperparameters\n",
    "decsion_tree_try_grid = [{\"max_depth\":depths,\n",
    "                          \"max_leaf_nodes\":max_leafs}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary of models for grid search CV\n",
    "DTM = {}\n",
    "\n",
    "for year in years:\n",
    "    DTM[f\"year{year}\"] = GridSearchCV(DecisionTreeClassifier(), param_grid=decsion_tree_try_grid, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the training set\n",
    "for year in years:\n",
    "    DTM[f\"year{year}\"].fit(X_train[f\"year{year}\"], Y_train[f\"year{year}\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following table summaries the \"best\" hyper-parameters and accruacy score for each year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = []\n",
    "max_leaf_nodes = []\n",
    "best_score = []\n",
    "\n",
    "for year in years:\n",
    "    max_depth.append(DTM[f\"year{year}\"].best_params_[\"max_depth\"])\n",
    "    max_leaf_nodes.append(DTM[f\"year{year}\"].best_params_[\"max_leaf_nodes\"])\n",
    "    best_score.append(DTM[f\"year{year}\"].best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>max_leaf_nodes</th>\n",
       "      <th>best_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>15</td>\n",
       "      <td>0.976517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>0.974809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0.963819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>0.958892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>20</td>\n",
       "      <td>0.948390</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  max_depth  max_leaf_nodes  best_score\n",
       "0     1         13              15    0.976517\n",
       "1     2          6              10    0.974809\n",
       "2     3         10               5    0.963819\n",
       "3     4         11              15    0.958892\n",
       "4     5          7              20    0.948390"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {\"year\":years, \"max_depth\":max_depth, \"max_leaf_nodes\":max_leaf_nodes, \"best_score\":best_score}\n",
    "pd.DataFrame(data=d, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the final decision tree models according to the best parameters\n",
    "DTM_final = {}\n",
    "\n",
    "for year in years:\n",
    "    DTM_final[f\"year{year}\"] = DecisionTreeClassifier(max_depth = DTM[f\"year{year}\"].best_params_[\"max_depth\"],\n",
    "                                                      max_leaf_nodes = DTM[f\"year{year}\"].best_params_[\"max_leaf_nodes\"])\n",
    "    \n",
    "    DTM_final[f\"year{year}\"].fit(X_train[f\"year{year}\"], Y_train[f\"year{year}\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Model evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
